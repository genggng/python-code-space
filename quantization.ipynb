{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日\n",
    "资料来源：\n",
    "- 知乎专栏：https://www.zhihu.com/column/c_1258047709686231040\n",
    "- github仓库： https://github.com/Jermmy/pytorch-quantization-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = (max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))   #量化误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam(nn.Module):\n",
    "    # 就是将上面的代码进行了封装\n",
    "    # 继承Module是为了让量化（参数）成为网络的一部分\n",
    "    def __init__(self,num_bits=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bits = num_bits\n",
    "        scale = torch.tensor([], requires_grad=False)\n",
    "        zero_point = torch.tensor([], requires_grad=False)\n",
    "        min = torch.tensor([], requires_grad=False)\n",
    "        max = torch.tensor([], requires_grad=False)\n",
    "        \n",
    "        # 使用register_buffer保存量化参数有以下优点：\n",
    "        # 不产生梯度，不会被注册到parameters中，但也会保存到state_dict中\n",
    "        # 这样就能把模型权重和量化参数都独立地保存到模型里。\n",
    "        self.register_buffer('scale', scale)  \n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        self.register_buffer('min', min)\n",
    "        self.register_buffer('max', max)\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max.nelement() == 0 or self.max <tensor.max():\n",
    "            self.max.data = tensor.max().data   #使用data赋值，避免self.max对象本身\n",
    "        self.max.clamp_(min=0)  #保证self.max大于等于0\n",
    "\n",
    "        if self.min.nelement() == 0  or self.min >tensor.min():\n",
    "            self.min.data = tensor.min().data\n",
    "        self.min.clamp_(max=0)  #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(self.min, self.max, self.num_bits)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def dequantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n",
    "\n",
    "    # 从状态字典加载量化参数\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
    "        key_names = ['scale', 'zero_point', 'min', 'max']\n",
    "        for key in key_names:\n",
    "            value = getattr(self, key)\n",
    "            value.data = state_dict[prefix + key].data\n",
    "            state_dict.pop(prefix + key)\n",
    "\n",
    "    def __str__(self):\n",
    "        info = 'scale:%.10f '  % self.scale\n",
    "        info += 'zero_point:%d '  % self.zero_point\n",
    "        info += 'min:%.6f '  % self.min\n",
    "        info += 'max:%.6f'  % self.max\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([  0., 229., 255.,  77., 178.])\n",
      "q_parm: scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n",
      "q_parm state dict: OrderedDict([('scale', tensor(0.1310)), ('zero_point', tensor(76.)), ('min', tensor(-10.)), ('max', tensor(23.4000))])\n",
      "q_parm_new scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "q_parm.update(x)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_parm:\",q_parm)  #打印量化参数\n",
    "print(\"q_parm state dict:\",q_parm.state_dict()) #打印状态字典\n",
    "torch.save(q_parm.state_dict(),\"./q_parm.pt\")# 保存状态字典\n",
    "\n",
    "q_parm_new = QParam(num_bits=8)  #创建新的量化参数对象\n",
    "q_parm_new.load_state_dict(torch.load(\"./q_parm.pt\")) #从保存的状态字典中加载量化参数\n",
    "print(\"q_parm_new\",q_parm_new)  #打印量化参数，看是否一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import math\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "\n",
    "    def __init__(self,qi=True,qo=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        qi:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        qo: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n",
    "\n",
    "class FakeQuantize(Function):\n",
    "    # 伪量化节点，进行量化和反量化\n",
    "    # 模拟量化前后的误差，这样的float推理和量化后的int推理具有相同的精度\n",
    "\n",
    "    # 反向传播求梯度使用STE，这部分在PTQ不进行反向传播，暂时可以忽略。  \n",
    "    # Function类似于没有参数的Module，继承需要重写前向传播forward和反向传播backward\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        \"\"\"\n",
    "        def forward(ctx,input,*args)\n",
    "            ctx: 不需要手动传入，能够执行一些操作方便求梯度，例如：\n",
    "            ctx.save_for_backward(tensor)  在前向传播时保存一些张量\n",
    "            tensor = ctx.saved_tensors    在反向传播时（backward函数内）也可访问到\n",
    "\n",
    "            input:函数的输入\n",
    "            *args:其他可选参数\n",
    "        \"\"\"\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output:后一层传来的梯度\n",
    "\n",
    "        在QAT涉及到反向传播\n",
    "        对于本层（伪量化节点）,不计算梯度，直接把后一层的梯度往前传\n",
    "\n",
    "        backward的返回值需要和forward对应，代表对应输入的梯度。\n",
    "        由于q_parm为量化参数，不需要计算梯度，因此返回None\n",
    "        因此直接 return grad_output，None\n",
    "        \"\"\"\n",
    "        return grad_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(qi=qi,qo=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "        self.num_bits = num_bits\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其冻结\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  为什么减去zero_point? 其实就是对应公式里 q_w-Z_w\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "        # 推理过程中顺便统计计算输入x，输出a，权重w的量化参数\n",
    "        # 后量化通过数据校准实现这些。\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qi)  #对q_x进行伪量化\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 对卷积权重q_w进行伪量化，然后和x计算卷积操作\n",
    "        # 注意卷积模块的权重始终是存储原始的weight，只有在前向传播时进行伪量化。\n",
    "        # 反向传播时也是更新的量化前的weight\n",
    "        x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, \n",
    "                     stride=self.conv_module.stride,\n",
    "                     padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                     groups=self.conv_module.groups)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qo)  #对输出a做伪量化\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point  #q_x - Z_x\n",
    "        x = self.conv_module(x)    #(q_w-Z_w)*(q_x-Z_x)\n",
    "        x = self.M * x             #M*sum((q_w-Z_w)*(q_x-Z_x))\n",
    "        x.round_()                 #提前做一步round，加快速度。后面只剩下一个整数加法。\n",
    "        x = x + self.qo.zero_point   #M*sum((q_w-Z_w)*(q_x-Z_x))+Z_a\n",
    "        x.round_()\n",
    "        return x\n",
    "\n",
    "# 相同原理 实现其他网络模块\n",
    "class QLinear(QModule):\n",
    "    #量化线性层\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight.data = self.qw.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.qi.scale * self.qw.scale,\n",
    "                                                   zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight.data)\n",
    "\n",
    "        x = F.linear(x, FakeQuantize.apply(self.fc_module.weight, self.qw), self.fc_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()\n",
    "        return x\n",
    "\n",
    "\n",
    "class QReLU(QModule):\n",
    "    # 构建量化版的Relu函数\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        \n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n",
    "\n",
    "class QMaxPooling2d(QModule):\n",
    "    # 构建量化版的MaxPooling2d函数\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv-BN-Relu融合层的实现\n",
    "一个标准的Conv+BN+relu可以合并到一起，加快推理速度。\n",
    "- **BN折叠到卷积层**  \n",
    "卷积层的输出如下所示:  $$y=\\sum_{i}^{N} w_{i} x_{i}+b$$\n",
    "BN层的输出如下所示:  \n",
    "$$y_{bn} =\\gamma \\frac{y-\\mu_{y}}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}+\\beta$$\n",
    "将卷积层的输出$y$带入到BN层中得到：  \n",
    "$$y_{b n}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}\\left(\\sum_{i}^{N} w_{i} x_{i}+b-\\mu_{y}\\right)+\\beta$$\n",
    "仔细观察，当训练结束后，BN层统计量$\\mu_{y}$ ,$\\sigma_{y}$以及参数$\\gamma$, $\\beta$都已经固定下来。我们可以令$\\gamma^{\\prime}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}$,那么能够得到：\n",
    "$$y_{b n}=\\sum_{i}^{N} \\gamma^{\\prime} w_{i} x_{i}+\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$$\n",
    "上式已经和卷积计算公式非常像了，为了看得更清楚，我们令$w_{i}^{\\prime} = \\gamma^{\\prime}w_{i}$,  $b^{\\prime}=\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$,得到最终的BN层输出\n",
    " $$y_{bn}=\\sum_{i}^{N} w_{i} x_{i}^{\\prime}+b^{\\prime}$$\n",
    "这就和卷积的操作一模一样，因此我们可以把BN层折叠合并到卷积层中。先进行数值变换，在进行矩阵运算。\n",
    "\n",
    "- **Relu折叠到卷积层**  \n",
    "1. 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。  \n",
    "2. 要想保证浮点型Relu和量化型Relu数值的一致性，需要对输入输出使用相同的量化参数,否则无法反量化回正确的实数域。  \n",
    "3. relu会对数值进行截断，实际上在量化过程中也存在截断，就是把scale和偏移zero point后的值截断到（qmin,qmax）。  \n",
    "4. 对于单独的一个Relu层，我们可以直接使用输入的qi在量化数值空间做截断，$relu(q_{x}) = max(q_{x},Z_{x})$。\n",
    "5. 对于合并到conv中的relu层，我们可以使用**输出的qo作为量化参数，对relu的输入x进行量化**，量化的截断过程可以直接实现relu的操作。这是因为在float域，relu的值域是[0,r_max]，对应的量化值域就是[q_min,q_max]。relu函数在float域中，对小于0的数置为0；在量化int域中，就是对小于q_min的数置为q_min。而使用qo对x进行量化，就能保证这一点。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    # 构建量化BN层（与Relu融合）\n",
    "    def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.qb = QParam(num_bits=32)\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def fold_bn(self, mean, std):\n",
    "        # 将BN层折叠到Conv层\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std  #r' = r/ sqrt(sigma^2+e)  实际上除以的标准差，加上e是为了避免分母为零\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) #w' = r'*w\n",
    "            if self.conv_module.bias is not None:         # b' = r' * b - r' * mu_y + beta\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias    #返回新的w和b\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        # 训练过程中，更新BN层参数\n",
    "        if self.training:\n",
    "            y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, \n",
    "                            stride=self.conv_module.stride,\n",
    "                            padding=self.conv_module.padding,\n",
    "                            dilation=self.conv_module.dilation,\n",
    "                            groups=self.conv_module.groups)\n",
    "            y = y.permute(1, 0, 2, 3) # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -> C,NHW\n",
    "            # mean = y.mean(1)\n",
    "            # var = y.var(1)\n",
    "            mean = y.mean(1).detach()\n",
    "            var = y.var(1).detach()\n",
    "            self.bn_module.running_mean = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                (1 - self.bn_module.momentum) * var\n",
    "        else:\n",
    "            mean = Variable(self.bn_module.running_mean)\n",
    "            var = Variable(self.bn_module.running_var)\n",
    "\n",
    "        std = torch.sqrt(var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(mean, std)  #折叠后的w和b\n",
    "\n",
    "        self.qw.update(weight.data)\n",
    "\n",
    "        #这里的卷积运算实际上包含了BN层\n",
    "        x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, \n",
    "                stride=self.conv_module.stride,\n",
    "                padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                groups=self.conv_module.groups)\n",
    "\n",
    "        x = F.relu(x)    #后面接上relu函数\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        std = torch.sqrt(self.bn_module.running_var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(self.bn_module.running_mean, std)\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(weight.data)\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias.data = quantize_tensor(bias, scale=self.qi.scale * self.qw.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        \n",
    "        #推理时将Relu合并到量化过程\n",
    "        #这一行包含了relu函数的作用，将x截断到(qmin,qmax)非对称量化         \n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型的构建\n",
    "我们在上面已经得到了基本的网络模块（卷积、池化、线性层等）的量化版本，  \n",
    "现在我们尝试构建一个完整的带有BN层的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    # 定义简单的卷积神经网络，包含两层卷积、2层BN层，1层全连接层\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    # 普通的全精度前向传播，穿插了2次relu函数和最大池化操作\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 我们需要把卷积层、池化层、全连接层转为其量化版本\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    # 量化版本的前向传播，就是各量化网络模块forward的串行堆叠\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    # 对各种量化参数进行固定\n",
    "    # 除了第一层卷积需要qi，其他都是对上一层的qo复用为当前层的qi，因为上一层的输出就是这一层的输入。\n",
    "    # 这里可以优化，伪量化过程中，这一层输出与下一层输入是一个tensor，但是执行了两次统计。\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.qo)\n",
    "        self.qconv2.freeze(qi=self.qconv1.qo)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.qo)\n",
    "        self.qfc.freeze(qi=self.qconv2.qo)\n",
    "\n",
    "    # 实际推理中用到的函数，是对各量化模块的quantize_inference的堆叠，纯整数存储和运算\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.qi.quantize_tensor(x) #对最开始的float输入进行量化\n",
    "\n",
    "        \"\"\"\n",
    "        中间推理都是在int域进行， 配合量化参数进行前向传播。\n",
    "        伪量化是在float域进行，虽有本质不同，但最终与int域反量化结果一致。\n",
    "        \"\"\"\n",
    "        qx = self.qconv1.quantize_inference(qx)    \n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.qo.dequantize_tensor(qx)   #最后一步要反量化，得到实数域的结果\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全精度模型训练\n",
    "这里直接使用仓库提供的 *train.py* 得到在mnist数据集上准确率为99%的FP32模型，权重保存在 *./ckpt/mnist_cnnbn.pt*  \n",
    "```shell\n",
    "python ./train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后训练量化（PTQ）\n",
    "这里会将一个全精度模型转为一个量化模型，并且测试其精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的全精度模型\n",
    "fp32_model = NetBN()\n",
    "fp32_model.load_state_dict(torch.load(\"./ckpt/mnist_cnnbn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是几个用于测试精度的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一定的图片数据进行校准，确定input和中间activation的量化参数\n",
    "# 实际上模型weight的校准并不需要数据，这里为了方便都放在quantize_forward中进行校准\n",
    "from torchvision import datasets,transforms\n",
    "test_batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def direct_quantize(model, test_loader):\n",
    "    # 直接量化，使用一些数据集来校准量化参数,使用200个batch的数据\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_forward(data)\n",
    "        if i % 200 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    # 全精度模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quantize_inference(model, test_loader):\n",
    "    # 量化模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        # data,target = data.to(device),target.to(device)\n",
    "        output = model.quantize_inference(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp32_model.eval()\n",
    "full_inference(fp32_model,test_loader)  #测试全精度的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n",
      "num_bits 1\n",
      "\n",
      "Test set: Quant Model Accuracy: 10%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 2\n",
      "\n",
      "Test set: Quant Model Accuracy: 29%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 3\n",
      "\n",
      "Test set: Quant Model Accuracy: 79%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 4\n",
      "\n",
      "Test set: Quant Model Accuracy: 98%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 5\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 6\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 7\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 8\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accuracy_list = []\n",
    "for num_bits in range(1,9):  #测试num_bits=[1-8]的准确率\n",
    "    model = copy.deepcopy(fp32_model)\n",
    "    model.quantize(num_bits=num_bits) # 将模型的各个网络模块转为其量化版本\n",
    "    model.eval()\n",
    "    direct_quantize(model,test_loader)  #进行量化校准\n",
    "    model.freeze()                       # 将量化参数固定\n",
    "    print(\"num_bits\",num_bits)\n",
    "    acc = quantize_inference(model, test_loader)  #测试量化准确率\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "acc_quants = pd.Series(accuracy_list,index=list(range(1,9)))\n",
    "acc_quants.plot()  #不同num_bits下的量化准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，当num_bits=4时，量化基本已经达到最大准确率99%。  \n",
    "这说明使用fp32存储这个小模型实在是浪费，使用int4即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.量化感知训练（QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型训练的问题\n",
    "量化模型我们插入了伪量化节点，经过反量化和量化，能够模拟量化带来的数值误差，实际存储的是反量化的float数值。  \n",
    "但是这样的模型还是不能训练，因为在计算图中，量化操作存在round函数（取整函数），这个函数**梯度几乎处处为0**。  \n",
    "这样我们就会导致反向传播的中梯度为0（链式法则），无法完成训练。  \n",
    "我们可以使用STE尝试解决这个问题。  \n",
    "#### Straight Through Estimator\n",
    "STE方式是，直接跳过伪量化的过程，避开round操作。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。  \n",
    "关键点：对weight伪量化会带来误差，从而降低模型精度。通过反向传播把梯度传给伪量化前的weight，并且**更新伪量化前的weight**， 使其适应量化带来的误差。这就是量化感知训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化感知训练的过程，和普通训练模型没什么差异\n",
    "# 更新的也是量化前weight，伪量化操作只在前向传播中使用引入量化误差，本身并不会修改weight\n",
    "def quantize_aware_training(model, device, train_loader, optimizer, epoch):\n",
    "    lossLayer = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.quantize_forward(data)\n",
    "        loss = lossLayer(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Quantize Aware Training Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize Aware Training Epoch: 1 [3200/60000]\tLoss: 0.366943\n",
      "Quantize Aware Training Epoch: 1 [6400/60000]\tLoss: 0.412441\n",
      "Quantize Aware Training Epoch: 1 [9600/60000]\tLoss: 0.301458\n",
      "Quantize Aware Training Epoch: 1 [12800/60000]\tLoss: 0.271548\n",
      "Quantize Aware Training Epoch: 1 [16000/60000]\tLoss: 0.350231\n",
      "Quantize Aware Training Epoch: 1 [19200/60000]\tLoss: 0.268980\n",
      "Quantize Aware Training Epoch: 1 [22400/60000]\tLoss: 0.465458\n",
      "Quantize Aware Training Epoch: 1 [25600/60000]\tLoss: 0.277720\n",
      "Quantize Aware Training Epoch: 1 [28800/60000]\tLoss: 0.408366\n",
      "Quantize Aware Training Epoch: 1 [32000/60000]\tLoss: 0.136120\n",
      "Quantize Aware Training Epoch: 1 [35200/60000]\tLoss: 0.270865\n",
      "Quantize Aware Training Epoch: 1 [38400/60000]\tLoss: 0.416413\n",
      "Quantize Aware Training Epoch: 1 [41600/60000]\tLoss: 0.326923\n",
      "Quantize Aware Training Epoch: 1 [44800/60000]\tLoss: 0.324175\n",
      "Quantize Aware Training Epoch: 1 [48000/60000]\tLoss: 0.320476\n",
      "Quantize Aware Training Epoch: 1 [51200/60000]\tLoss: 0.127868\n",
      "Quantize Aware Training Epoch: 1 [54400/60000]\tLoss: 0.325320\n",
      "Quantize Aware Training Epoch: 1 [57600/60000]\tLoss: 0.427918\n",
      "Quantize Aware Training Epoch: 2 [3200/60000]\tLoss: 0.391900\n",
      "Quantize Aware Training Epoch: 2 [6400/60000]\tLoss: 0.217050\n",
      "Quantize Aware Training Epoch: 2 [9600/60000]\tLoss: 0.283597\n",
      "Quantize Aware Training Epoch: 2 [12800/60000]\tLoss: 0.417715\n",
      "Quantize Aware Training Epoch: 2 [16000/60000]\tLoss: 0.205772\n",
      "Quantize Aware Training Epoch: 2 [19200/60000]\tLoss: 0.238495\n",
      "Quantize Aware Training Epoch: 2 [22400/60000]\tLoss: 0.255134\n",
      "Quantize Aware Training Epoch: 2 [25600/60000]\tLoss: 0.301560\n",
      "Quantize Aware Training Epoch: 2 [28800/60000]\tLoss: 0.277910\n",
      "Quantize Aware Training Epoch: 2 [32000/60000]\tLoss: 0.368340\n",
      "Quantize Aware Training Epoch: 2 [35200/60000]\tLoss: 0.283693\n",
      "Quantize Aware Training Epoch: 2 [38400/60000]\tLoss: 0.408710\n",
      "Quantize Aware Training Epoch: 2 [41600/60000]\tLoss: 0.341289\n",
      "Quantize Aware Training Epoch: 2 [44800/60000]\tLoss: 0.270558\n",
      "Quantize Aware Training Epoch: 2 [48000/60000]\tLoss: 0.371161\n",
      "Quantize Aware Training Epoch: 2 [51200/60000]\tLoss: 0.331406\n",
      "Quantize Aware Training Epoch: 2 [54400/60000]\tLoss: 0.585725\n",
      "Quantize Aware Training Epoch: 2 [57600/60000]\tLoss: 0.413231\n",
      "Quantize Aware Training Epoch: 3 [3200/60000]\tLoss: 0.302144\n",
      "Quantize Aware Training Epoch: 3 [6400/60000]\tLoss: 0.620318\n",
      "Quantize Aware Training Epoch: 3 [9600/60000]\tLoss: 0.274309\n",
      "Quantize Aware Training Epoch: 3 [12800/60000]\tLoss: 0.237570\n",
      "Quantize Aware Training Epoch: 3 [16000/60000]\tLoss: 0.652278\n",
      "Quantize Aware Training Epoch: 3 [19200/60000]\tLoss: 0.374038\n",
      "Quantize Aware Training Epoch: 3 [22400/60000]\tLoss: 0.257600\n",
      "Quantize Aware Training Epoch: 3 [25600/60000]\tLoss: 0.384755\n",
      "Quantize Aware Training Epoch: 3 [28800/60000]\tLoss: 0.361981\n",
      "Quantize Aware Training Epoch: 3 [32000/60000]\tLoss: 0.576575\n",
      "Quantize Aware Training Epoch: 3 [35200/60000]\tLoss: 0.515714\n",
      "Quantize Aware Training Epoch: 3 [38400/60000]\tLoss: 0.162003\n",
      "Quantize Aware Training Epoch: 3 [41600/60000]\tLoss: 0.399571\n",
      "Quantize Aware Training Epoch: 3 [44800/60000]\tLoss: 0.224104\n",
      "Quantize Aware Training Epoch: 3 [48000/60000]\tLoss: 0.544554\n",
      "Quantize Aware Training Epoch: 3 [51200/60000]\tLoss: 0.371413\n",
      "Quantize Aware Training Epoch: 3 [54400/60000]\tLoss: 0.337052\n",
      "Quantize Aware Training Epoch: 3 [57600/60000]\tLoss: 0.578013\n"
     ]
    }
   ],
   "source": [
    "# 进行量化感知训练\n",
    "from torch import optim\n",
    "batch_size = 64\n",
    "seed = 1\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False\n",
    ")\n",
    "\n",
    "model_qat = copy.deepcopy(fp32_model)  #从全精度模型获得量化模型\n",
    "model_qat.quantize(num_bits=3)  #量化位数为3\n",
    "model_qat.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "model_qat.train()\n",
    "for epoch in range(epochs):\n",
    "    quantize_aware_training(model_qat,device,train_loader,optimizer,epoch+1)\n",
    "model_qat.eval()\n",
    "model_qat.freeze() #冻结量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Quant Model Accuracy: 83%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.87"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat.cpu()\n",
    "quantize_inference(model_qat, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在num_bits=3时，PTQ就有84%的准确率，而QAT只有74%的准确率，还不如直接量化呢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.网络性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建追踪的属性\n",
    "model.eval() #测试量化感知模型的性能\n",
    "\n",
    "with torch.profiler.profiler(\n",
    "    schedule = torch.profiler.schedule(wait=2,warmup=2,active=6,repeat=1),\n",
    "    on_trace_ready = torch.profiler.tensorboard_trace_handler(dir_name=\"./performance/\"),  #日志保存地址\n",
    "    activities = [\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA\n",
    "    ],\n",
    "    with_stack = True,\n",
    ") as profiler:\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm(test_loader,desc=\"profiling...\"):  #这里用假数据也行，不考虑模型预测精度\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0127)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.qconv1.qi.buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = copy.deepcopy(fp32_model)  \n",
    "model_qat.quantize(num_bits=3)  #量化位数为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetBN(\n",
       "  (conv1): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (qconv1): QConvBNReLU(\n",
       "    (qi): QParam()\n",
       "    (qo): QParam()\n",
       "    (conv_module): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn_module): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (qw): QParam()\n",
       "    (qb): QParam()\n",
       "  )\n",
       "  (qmaxpool2d_1): QMaxPooling2d(\n",
       "    (qo): QParam()\n",
       "  )\n",
       "  (qconv2): QConvBNReLU(\n",
       "    (qo): QParam()\n",
       "    (conv_module): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn_module): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (qw): QParam()\n",
       "    (qb): QParam()\n",
       "  )\n",
       "  (qmaxpool2d_2): QMaxPooling2d(\n",
       "    (qo): QParam()\n",
       "  )\n",
       "  (qfc): QLinear(\n",
       "    (qo): QParam()\n",
       "    (fc_module): Linear(in_features=1000, out_features=10, bias=True)\n",
       "    (qw): QParam()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_qat.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QConvBNReLU' object has no attribute 'M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/python-code-space/quantization.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu69/home/geng/tinyml/python-code-space/quantization.ipynb#ch0000042vscode-remote?line=0'>1</a>\u001b[0m model_qat\u001b[39m.\u001b[39;49mqconv1\u001b[39m.\u001b[39;49mM\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/ppq/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QConvBNReLU' object has no attribute 'M'"
     ]
    }
   ],
   "source": [
    "model_qat.qconv1.M.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
