{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日\n",
    "资料来源：\n",
    "- 知乎专栏：https://www.zhihu.com/column/c_1258047709686231040\n",
    "- github仓库： https://github.com/Jermmy/pytorch-quantization-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = (max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))   #量化误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam(nn.Module):\n",
    "    # 就是将上面的代码进行了封装\n",
    "    # 继承Module是为了让量化（参数）成为网络的一部分\n",
    "    def __init__(self,num_bits=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bits = num_bits\n",
    "        scale = torch.tensor([], requires_grad=False)\n",
    "        zero_point = torch.tensor([], requires_grad=False)\n",
    "        min = torch.tensor([], requires_grad=False)\n",
    "        max = torch.tensor([], requires_grad=False)\n",
    "        \n",
    "        # 使用register_buffer保存量化参数有以下优点：\n",
    "        # 不产生梯度，不会被注册到parameters中，但也会保存到state_dict中\n",
    "        # 这样就能把模型权重和量化参数都独立地保存到模型里。\n",
    "        self.register_buffer('scale', scale)  \n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        self.register_buffer('min', min)\n",
    "        self.register_buffer('max', max)\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max.nelement() == 0 or self.max <tensor.max():\n",
    "            self.max.data = tensor.max().data   #使用data赋值，避免self.max对象本身\n",
    "        self.max.clamp_(min=0)  #保证self.max大于等于0\n",
    "\n",
    "        if self.min.nelement() == 0  or self.min >tensor.min():\n",
    "            self.min.data = tensor.min().data\n",
    "        self.min.clamp_(max=0)  #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(self.min, self.max, self.num_bits)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def dequantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n",
    "\n",
    "    # 从状态字典加载量化参数\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
    "        key_names = ['scale', 'zero_point', 'min', 'max']\n",
    "        for key in key_names:\n",
    "            value = getattr(self, key)\n",
    "            value.data = state_dict[prefix + key].data\n",
    "            state_dict.pop(prefix + key)\n",
    "\n",
    "    def __str__(self):\n",
    "        info = 'scale:%.10f '  % self.scale\n",
    "        info += 'zero_point:%d '  % self.zero_point\n",
    "        info += 'min:%.6f '  % self.min\n",
    "        info += 'max:%.6f'  % self.max\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([  0., 229., 255.,  77., 178.])\n",
      "q_parm: scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n",
      "q_parm state dict: OrderedDict([('scale', tensor(0.1310)), ('zero_point', tensor(76.)), ('min', tensor(-10.)), ('max', tensor(23.4000))])\n",
      "q_parm_new scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "q_parm.update(x)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_parm:\",q_parm)  #打印量化参数\n",
    "print(\"q_parm state dict:\",q_parm.state_dict()) #打印状态字典\n",
    "torch.save(q_parm.state_dict(),\"./q_parm.pt\")# 保存状态字典\n",
    "\n",
    "q_parm_new = QParam(num_bits=8)  #创建新的量化参数对象\n",
    "q_parm_new.load_state_dict(torch.load(\"./q_parm.pt\")) #从保存的状态字典中加载量化参数\n",
    "print(\"q_parm_new\",q_parm_new)  #打印量化参数，看是否一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import math\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "\n",
    "    def __init__(self,qi=True,qo=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        qi:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        qo: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n",
    "\n",
    "class FakeQuantize(Function):\n",
    "    # 伪量化节点，进行量化和反量化\n",
    "    # 模拟量化前后的误差，这样的float推理和量化后的int推理具有相同的精度\n",
    "\n",
    "    # 反向传播求梯度使用STE，这部分在PTQ不进行反向传播，暂时可以忽略。  \n",
    "    # Function类似于没有参数的Module，继承需要重写前向传播forward和反向传播backward\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        \"\"\"\n",
    "        def forward(ctx,input,*args)\n",
    "            ctx: 不需要手动传入，能够执行一些操作方便求梯度，例如：\n",
    "            ctx.save_for_backward(tensor)  在前向传播时保存一些张量\n",
    "            tensor = ctx.saved_tensors    在反向传播时（backward函数内）也可访问到\n",
    "\n",
    "            input:函数的输入\n",
    "            *args:其他可选参数\n",
    "        \"\"\"\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output:后一层传来的梯度\n",
    "\n",
    "        在QAT涉及到反向传播\n",
    "        对于本层（伪量化节点）,不计算梯度，直接把后一层的梯度往前传\n",
    "\n",
    "        backward的返回值需要和forward对应，代表对应输入的梯度。\n",
    "        由于q_parm为量化参数，不需要计算梯度，因此返回None\n",
    "        因此直接 return grad_output，None\n",
    "        \"\"\"\n",
    "        return grad_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(qi=qi,qo=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "        self.num_bits = num_bits\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其冻结\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  为什么减去zero_point? 其实就是对应公式里 q_w-Z_w\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "        # 推理过程中顺便统计计算输入x，输出a，权重w的量化参数\n",
    "        # 后量化通过数据校准实现这些。\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qi)  #对q_x进行伪量化\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 对卷积权重q_w进行伪量化，然后和x计算卷积操作\n",
    "        # 注意卷积模块的权重始终是存储原始的weight，只有在前向传播时进行伪量化。\n",
    "        # 反向传播时也是更新的量化前的weight\n",
    "        x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, \n",
    "                     stride=self.conv_module.stride,\n",
    "                     padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                     groups=self.conv_module.groups)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qo)  #对输出a做伪量化\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point  #q_x - Z_x\n",
    "        x = self.conv_module(x)    #(q_w-Z_w)*(q_x-Z_x)\n",
    "        x = self.M * x             #M*sum((q_w-Z_w)*(q_x-Z_x))\n",
    "        x.round_()                 #提前做一步round，加快速度。后面只剩下一个整数加法。\n",
    "        x = x + self.qo.zero_point   #M*sum((q_w-Z_w)*(q_x-Z_x))+Z_a\n",
    "        x.round_()\n",
    "        return x\n",
    "\n",
    "# 相同原理 实现其他网络模块\n",
    "class QLinear(QModule):\n",
    "    #量化线性层\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight.data = self.qw.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.qi.scale * self.qw.scale,\n",
    "                                                   zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight.data)\n",
    "\n",
    "        x = F.linear(x, FakeQuantize.apply(self.fc_module.weight, self.qw), self.fc_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()\n",
    "        return x\n",
    "\n",
    "\n",
    "class QReLU(QModule):\n",
    "    # 构建量化版的Relu函数\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        \n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n",
    "\n",
    "class QMaxPooling2d(QModule):\n",
    "    # 构建量化版的MaxPooling2d函数\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv-BN-Relu融合层的实现\n",
    "一个标准的Conv+BN+relu可以合并到一起，加快推理速度。\n",
    "- **BN折叠到卷积层**  \n",
    "卷积层的输出如下所示:  $$y=\\sum_{i}^{N} w_{i} x_{i}+b$$\n",
    "BN层的输出如下所示:  \n",
    "$$y_{bn} =\\gamma \\frac{y-\\mu_{y}}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}+\\beta$$\n",
    "将卷积层的输出$y$带入到BN层中得到：  \n",
    "$$y_{b n}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}\\left(\\sum_{i}^{N} w_{i} x_{i}+b-\\mu_{y}\\right)+\\beta$$\n",
    "仔细观察，当训练结束后，BN层统计量$\\mu_{y}$ ,$\\sigma_{y}$以及参数$\\gamma$, $\\beta$都已经固定下来。我们可以令$\\gamma^{\\prime}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}$,那么能够得到：\n",
    "$$y_{b n}=\\sum_{i}^{N} \\gamma^{\\prime} w_{i} x_{i}+\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$$\n",
    "上式已经和卷积计算公式非常像了，为了看得更清楚，我们令$w_{i}^{\\prime} = \\gamma^{\\prime}w_{i}$,  $b^{\\prime}=\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$,得到最终的BN层输出\n",
    " $$y_{bn}=\\sum_{i}^{N} w_{i} x_{i}^{\\prime}+b^{\\prime}$$\n",
    "这就和卷积的操作一模一样，因此我们可以把BN层折叠合并到卷积层中。先进行数值变换，在进行矩阵运算。\n",
    "\n",
    "- **Relu折叠到卷积层**  \n",
    "1. 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。  \n",
    "2. 要想保证浮点型Relu和量化型Relu数值的一致性，需要对输入输出使用相同的量化参数,否则无法反量化回正确的实数域。  \n",
    "3. relu会对数值进行截断，实际上在量化过程中也存在截断，就是把scale和偏移zero point后的值截断到（qmin,qmax）。  \n",
    "4. 对于单独的一个Relu层，我们可以直接使用输入的qi在量化数值空间做截断，$relu(q_{x}) = max(q_{x},Z_{x})$。\n",
    "5. 对于合并到conv中的relu层，我们可以使用**输出的qo作为量化参数，对relu的输入x进行量化**，量化的截断过程可以直接实现relu的操作。这是因为在float域，relu的值域是[0,r_max]，对应的量化值域就是[q_min,q_max]。relu函数在float域中，对小于0的数置为0；在量化int域中，就是对小于q_min的数置为q_min。而使用qo对x进行量化，就能保证这一点。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    # 构建量化BN层（与Relu融合）\n",
    "    def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.qb = QParam(num_bits=32)\n",
    "        M = torch.tensor([], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def fold_bn(self, mean, std):\n",
    "        # 将BN层折叠到Conv层\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std  #r' = r/ sqrt(sigma^2+e)  实际上除以的标准差，加上e是为了避免分母为零\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) #w' = r'*w\n",
    "            if self.conv_module.bias is not None:         # b' = r' * b - r' * mu_y + beta\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias    #返回新的w和b\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        # 训练过程中，更新BN层参数\n",
    "        if self.training:\n",
    "            y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, \n",
    "                            stride=self.conv_module.stride,\n",
    "                            padding=self.conv_module.padding,\n",
    "                            dilation=self.conv_module.dilation,\n",
    "                            groups=self.conv_module.groups)\n",
    "            y = y.permute(1, 0, 2, 3) # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -> C,NHW\n",
    "            # mean = y.mean(1)\n",
    "            # var = y.var(1)\n",
    "            mean = y.mean(1).detach()\n",
    "            var = y.var(1).detach()\n",
    "            self.bn_module.running_mean = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                (1 - self.bn_module.momentum) * var\n",
    "        else:\n",
    "            mean = Variable(self.bn_module.running_mean)\n",
    "            var = Variable(self.bn_module.running_var)\n",
    "\n",
    "        std = torch.sqrt(var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(mean, std)  #折叠后的w和b\n",
    "\n",
    "        self.qw.update(weight.data)\n",
    "\n",
    "        #这里的卷积运算实际上包含了BN层\n",
    "        x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, \n",
    "                stride=self.conv_module.stride,\n",
    "                padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                groups=self.conv_module.groups)\n",
    "\n",
    "        x = F.relu(x)    #后面接上relu函数\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        std = torch.sqrt(self.bn_module.running_var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(self.bn_module.running_mean, std)\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(weight.data)\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias.data = quantize_tensor(bias, scale=self.qi.scale * self.qw.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        \n",
    "        #推理时将Relu合并到量化过程\n",
    "        #这一行包含了relu函数的作用，将x截断到(qmin,qmax)非对称量化         \n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型的构建\n",
    "我们在上面已经得到了基本的网络模块（卷积、池化、线性层等）的量化版本，  \n",
    "现在我们尝试构建一个完整的带有BN层的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    # 定义简单的卷积神经网络，包含两层卷积、2层BN层，1层全连接层\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    # 普通的全精度前向传播，穿插了2次relu函数和最大池化操作\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 我们需要把卷积层、池化层、全连接层转为其量化版本\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    # 量化版本的前向传播，就是各量化网络模块forward的串行堆叠\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    # 对各种量化参数进行固定\n",
    "    # 除了第一层卷积需要qi，其他都是对上一层的qo复用为当前层的qi，因为上一层的输出就是这一层的输入。\n",
    "    # 这里可以优化，伪量化过程中，这一层输出与下一层输入是一个tensor，但是执行了两次统计。\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.qo)\n",
    "        self.qconv2.freeze(qi=self.qconv1.qo)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.qo)\n",
    "        self.qfc.freeze(qi=self.qconv2.qo)\n",
    "\n",
    "    # 实际推理中用到的函数，是对各量化模块的quantize_inference的堆叠，纯整数存储和运算\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.qi.quantize_tensor(x) #对最开始的float输入进行量化\n",
    "\n",
    "        \"\"\"\n",
    "        中间推理都是在int域进行， 配合量化参数进行前向传播。\n",
    "        伪量化是在float域进行，虽有本质不同，但最终与int域反量化结果一致。\n",
    "        \"\"\"\n",
    "        qx = self.qconv1.quantize_inference(qx)    \n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.qo.dequantize_tensor(qx)   #最后一步要反量化，得到实数域的结果\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全精度模型训练\n",
    "这里直接使用仓库提供的 *train.py* 得到在mnist数据集上准确率为99%的FP32模型，权重保存在 *./ckpt/mnist_cnnbn.pt*  \n",
    "```shell\n",
    "python ./train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后训练量化（PTQ）\n",
    "这里会将一个全精度模型转为一个量化模型，并且测试其精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的全精度模型\n",
    "fp32_model = NetBN()\n",
    "fp32_model.load_state_dict(torch.load(\"./ckpt/mnist_cnnbn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是几个用于测试精度的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一定的图片数据进行校准，确定input和中间activation的量化参数\n",
    "# 实际上模型weight的校准并不需要数据，这里为了方便都放在quantize_forward中进行校准\n",
    "from torchvision import datasets,transforms\n",
    "test_batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def direct_quantize(model, test_loader):\n",
    "    # 直接量化，使用一些数据集来校准量化参数,使用200个batch的数据\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_forward(data)\n",
    "        if i % 200 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    # 全精度模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quantize_inference(model, test_loader,device):\n",
    "    # 量化模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        output = model.quantize_inference(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp32_model.eval()\n",
    "full_inference(fp32_model,test_loader)  #测试全精度的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n",
      "num_bits 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "quantize_inference() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/geng/tinyml/python-code-space/Notes.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu69/home/geng/tinyml/python-code-space/Notes.ipynb#ch0000026vscode-remote?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mfreeze()                       \u001b[39m# 将量化参数固定\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu69/home/geng/tinyml/python-code-space/Notes.ipynb#ch0000026vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnum_bits\u001b[39m\u001b[39m\"\u001b[39m,num_bits)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu69/home/geng/tinyml/python-code-space/Notes.ipynb#ch0000026vscode-remote?line=9'>10</a>\u001b[0m acc \u001b[39m=\u001b[39m quantize_inference(model, test_loader)  \u001b[39m#测试量化准确率\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu69/home/geng/tinyml/python-code-space/Notes.ipynb#ch0000026vscode-remote?line=10'>11</a>\u001b[0m accuracy_list\u001b[39m.\u001b[39mappend(acc)\n",
      "\u001b[0;31mTypeError\u001b[0m: quantize_inference() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accuracy_list = []\n",
    "for num_bits in range(1,9):  #测试num_bits=[1-8]的准确率\n",
    "    model = copy.deepcopy(fp32_model)\n",
    "    model.quantize(num_bits=num_bits) # 将模型的各个网络模块转为其量化版本\n",
    "    model.eval()\n",
    "    direct_quantize(model,test_loader)  #进行量化校准\n",
    "    model.freeze()                       # 将量化参数固定\n",
    "    print(\"num_bits\",num_bits)\n",
    "    acc = quantize_inference(model, test_loader)  #测试量化准确率\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "acc_quants = pd.Series(accuracy_list,index=list(range(1,9)))\n",
    "acc_quants.plot()  #不同num_bits下的量化准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，当num_bits=4时，量化基本已经达到最大准确率99%。  \n",
    "这说明使用fp32存储这个小模型实在是浪费，使用int4即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.量化感知训练（QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型训练的问题\n",
    "量化模型我们插入了伪量化节点，经过反量化和量化，能够模拟量化带来的数值误差，实际存储的是反量化的float数值。  \n",
    "但是这样的模型还是不能训练，因为在计算图中，量化操作存在round函数（取整函数），这个函数**梯度几乎处处为0**。  \n",
    "这样我们就会导致反向传播的中梯度为0（链式法则），无法完成训练。  \n",
    "我们可以使用STE尝试解决这个问题。  \n",
    "#### Straight Through Estimator\n",
    "STE方式是，直接跳过伪量化的过程，避开round操作。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。  \n",
    "关键点：对weight伪量化会带来误差，从而降低模型精度。通过反向传播把梯度传给伪量化前的weight，并且**更新伪量化前的weight**， 使其适应量化带来的误差。这就是量化感知训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化感知训练的过程，和普通训练模型没什么差异\n",
    "# 更新的也是量化前weight，伪量化操作只在前向传播中使用引入量化误差，本身并不会修改weight\n",
    "def quantize_aware_training(model, device, train_loader, optimizer, epoch):\n",
    "    lossLayer = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.quantize_forward(data)\n",
    "        loss = lossLayer(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Quantize Aware Training Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize Aware Training Epoch: 1 [3200/60000]\tLoss: 0.281344\n",
      "Quantize Aware Training Epoch: 1 [6400/60000]\tLoss: 0.376364\n",
      "Quantize Aware Training Epoch: 1 [9600/60000]\tLoss: 0.330406\n",
      "Quantize Aware Training Epoch: 1 [12800/60000]\tLoss: 0.312176\n",
      "Quantize Aware Training Epoch: 1 [16000/60000]\tLoss: 0.254599\n",
      "Quantize Aware Training Epoch: 1 [19200/60000]\tLoss: 0.261912\n",
      "Quantize Aware Training Epoch: 1 [22400/60000]\tLoss: 0.260504\n",
      "Quantize Aware Training Epoch: 1 [25600/60000]\tLoss: 0.364713\n",
      "Quantize Aware Training Epoch: 1 [28800/60000]\tLoss: 0.278515\n",
      "Quantize Aware Training Epoch: 1 [32000/60000]\tLoss: 0.298310\n",
      "Quantize Aware Training Epoch: 1 [35200/60000]\tLoss: 0.493630\n",
      "Quantize Aware Training Epoch: 1 [38400/60000]\tLoss: 0.220398\n",
      "Quantize Aware Training Epoch: 1 [41600/60000]\tLoss: 0.300470\n",
      "Quantize Aware Training Epoch: 1 [44800/60000]\tLoss: 0.162521\n",
      "Quantize Aware Training Epoch: 1 [48000/60000]\tLoss: 0.400347\n",
      "Quantize Aware Training Epoch: 1 [51200/60000]\tLoss: 0.477783\n",
      "Quantize Aware Training Epoch: 1 [54400/60000]\tLoss: 0.300301\n",
      "Quantize Aware Training Epoch: 1 [57600/60000]\tLoss: 0.241377\n",
      "Quantize Aware Training Epoch: 2 [3200/60000]\tLoss: 0.350297\n",
      "Quantize Aware Training Epoch: 2 [6400/60000]\tLoss: 0.351356\n",
      "Quantize Aware Training Epoch: 2 [9600/60000]\tLoss: 0.345339\n",
      "Quantize Aware Training Epoch: 2 [12800/60000]\tLoss: 0.300483\n",
      "Quantize Aware Training Epoch: 2 [16000/60000]\tLoss: 0.467272\n",
      "Quantize Aware Training Epoch: 2 [19200/60000]\tLoss: 0.224360\n",
      "Quantize Aware Training Epoch: 2 [22400/60000]\tLoss: 0.487409\n",
      "Quantize Aware Training Epoch: 2 [25600/60000]\tLoss: 0.334949\n",
      "Quantize Aware Training Epoch: 2 [28800/60000]\tLoss: 0.323530\n",
      "Quantize Aware Training Epoch: 2 [32000/60000]\tLoss: 0.317323\n",
      "Quantize Aware Training Epoch: 2 [35200/60000]\tLoss: 0.524773\n",
      "Quantize Aware Training Epoch: 2 [38400/60000]\tLoss: 0.518919\n"
     ]
    }
   ],
   "source": [
    "# 进行量化感知训练\n",
    "from torch import optim\n",
    "batch_size = 64\n",
    "seed = 1\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False\n",
    ")\n",
    "\n",
    "model_qat = copy.deepcopy(fp32_model)  #从全精度模型获得量化模型\n",
    "model_qat.quantize(num_bits=3)  #量化位数为3\n",
    "model_qat.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "model_qat.train()\n",
    "for epoch in range(epochs):\n",
    "    quantize_aware_training(model_qat,device,train_loader,optimizer,epoch+1)\n",
    "model_qat.eval()\n",
    "model_qat.freeze() #冻结量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Quant Model Accuracy: 97%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.37"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_inference(model_qat, test_loader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在num_bits=3时，PTQ就有84%的准确率，而QAT只有74%的准确率，还不如直接量化呢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.网络性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建追踪的属性\n",
    "model.eval() #测试量化感知模型的性能\n",
    "\n",
    "with torch.profiler.profiler(\n",
    "    schedule = torch.profiler.schedule(wait=2,warmup=2,active=6,repeat=1),\n",
    "    on_trace_ready = torch.profiler.tensorboard_trace_handler(dir_name=\"./performance/\"),  #日志保存地址\n",
    "    activities = [\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA\n",
    "    ],\n",
    "    with_stack = True,\n",
    ") as profiler:\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm(test_loader,desc=\"profiling...\"):  #这里用假数据也行，不考虑模型预测精度\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0127)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.qconv1.qi.buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
