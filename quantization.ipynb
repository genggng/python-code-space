{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化的简单实现\n",
    "主要包括后量化（PTQ）和感知量化（QAT）  \n",
    "作者：genggng  日期：2022年6月29日\n",
    "资料来源：\n",
    "- 知乎专栏：https://www.zhihu.com/column/c_1258047709686231040\n",
    "- github仓库： https://github.com/Jermmy/pytorch-quantization-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 后量化的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化的讲实数转为低比特的整数，转换公式为：\n",
    "$$r= S(q-Z)$$\n",
    "$$q=round(\\frac{r}{S}+Z)$$\n",
    "后量化的关键就是计算出scale（实数和整数的放缩比例）和zero point（实数0量化后对应的整数）.\n",
    "$$S=\\frac{r_{\\max }-r_{\\min }}{q_{\\max }-q_{\\min }}$$\n",
    "$$ Z = round(q_{\\max}-\\frac{r_{max}}{S})$$\n",
    "下面使用代码实现这两部分完成基本的tensor量化\n",
    "当出现$Z>q_{\\max}$或$Z<q_{\\min}$ 时，需要对Z进行截断（因为Z也是用uint存储的)。\n",
    "此时推导可知$r_{\\max}<0$ 或 $r_{\\min}>0$ ，因此应该**尽量避免tensor全为正数或者负数的情况**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本量化操作的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaleZeroPoint(min_val,max_val,num_bits=8):\n",
    "    \"\"\"\n",
    "    计算量化参数scale和zero point\n",
    "    @param\n",
    "        min_val: 实数最大值\n",
    "        max_val:  实数最小值\n",
    "        num_bits:  量化位数\n",
    "    \n",
    "    @return\n",
    "        scale: 实数整数放缩比例\n",
    "        zero_point: 量化后的零点\n",
    "    \"\"\"\n",
    "    #注意这里输入的mix_val，max_val是标量。\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \"\"\"\n",
    "    这里主要是用到qmax和qmin的差值。\n",
    "    实数和量化数的范围比例为scale，\n",
    "    rmax放缩后的数和qmax差值就是zero point.\n",
    "    所以q_min和q_max本身数值并不重要。\n",
    "    \"\"\"   \n",
    "    scale = (max_val-min_val) / (q_max-q_min)\n",
    "    zero_point = q_max - max_val/scale\n",
    "\n",
    "    #为什么要截断zero_point?,因为零点也是用uint8存储的。\n",
    "    if zero_point < q_min:\n",
    "        zero_point = torch.tensor([q_min], dtype=torch.float32).to(min_val.device)\n",
    "    elif zero_point > q_max:\n",
    "        # zero_point = qmax\n",
    "        zero_point = torch.tensor([q_max], dtype=torch.float32).to(max_val.device)\n",
    "    \n",
    "    zero_point.round_()\n",
    "\n",
    "    return scale,zero_point\n",
    "\n",
    "def quantize_tensor(x,scale,zero_point,num_bits=8,signed=False):\n",
    "    \"\"\"\n",
    "    对张量x进行量化\n",
    "    @param:\n",
    "        x:待量化浮点数张量\n",
    "        scale,zero_point:量化参数\n",
    "        num_bits:量化位数\n",
    "        signed:采用有符号量化\n",
    "    @return:\n",
    "        q_x:量化为整数的张量\n",
    "    \"\"\"\n",
    "    if signed: #量化到有符号数[-128,127]\n",
    "        q_min = - 2. ** (num_bits-1)\n",
    "        q_max = 2. ** (num_bits-1) - 1\n",
    "    else:  #量化到无符号数[0,255]\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits - 1 \n",
    "    \n",
    "    q_x = x/scale + zero_point\n",
    "    q_x.clamp_(q_min,q_max).round_() #使用pytorch内置函数进行截断和四舍五入取整。这一行相当于公式round函数\n",
    "\n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x,scale,zero_point):\n",
    "    \"\"\"\n",
    "    将量化后的张量q_x反量化为浮点张量x\n",
    "    @param:\n",
    "        q_x:量化为整数的张量\n",
    "        scale,zero_point:量化参数\n",
    "    return:\n",
    "        量化之前的浮点张量x\n",
    "    \"\"\"\n",
    "    return scale * (q_x - zero_point) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=0.131,zero_point=76.0\n",
      "q_x=tensor([  0., 229., 255.,  77., 178.])\n",
      "deq_x=tensor([-9.9545, 20.0400, 23.4455,  0.1310, 13.3600])\n",
      "error=tensor([ 0.0455, -0.0600,  0.0455,  0.0310,  0.0600])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "scale,zero_point = getScaleZeroPoint(x.min(),x.max(),8)\n",
    "q_x = quantize_tensor(x,scale,zero_point)\n",
    "deq_x = dequantize_tensor(q_x,scale,zero_point)\n",
    "\n",
    "print(\"scale={:.3f},zero_point={}\".format(scale,zero_point))\n",
    "print(\"q_x={}\".format(q_x))\n",
    "print(\"deq_x={}\".format(deq_x))\n",
    "print(\"error={}\".format(deq_x-x))   #量化误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化参数类的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在量化过程中，需要统计权重和激活值张量的max-min信息，并计算对应的scale和zero point，从而执行量化操作。  \n",
    "我们可以将要保存的参数和要使用的量化操作封装为一个类，即量化参数。  \n",
    "量化算法的关键也是量化参数的确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam(nn.Module):\n",
    "    # 就是将上面的代码进行了封装\n",
    "    # 继承Module是为了让量化（参数）成为网络的一部分\n",
    "    def __init__(self,num_bits=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_bits = num_bits\n",
    "        scale = torch.tensor([], requires_grad=False)\n",
    "        zero_point = torch.tensor([], requires_grad=False)\n",
    "        min = torch.tensor([], requires_grad=False)\n",
    "        max = torch.tensor([], requires_grad=False)\n",
    "        \n",
    "        # 使用register_buffer保存量化参数有以下优点：\n",
    "        # 不产生梯度，不会被注册到parameters中，但也会保存到state_dict中\n",
    "        # 这样就能把模型权重和量化参数都独立地保存到模型里。\n",
    "        self.register_buffer('scale', scale)  \n",
    "        self.register_buffer('zero_point', zero_point)\n",
    "        self.register_buffer('min', min)\n",
    "        self.register_buffer('max', max)\n",
    "        \n",
    "    def update(self,tensor):\n",
    "        # 对于输入的待量化张量，更新对应的量化参数\n",
    "        if self.max.nelement() == 0 or self.max <tensor.max():\n",
    "            self.max.data = tensor.max().data   #使用data赋值，避免self.max对象本身\n",
    "        self.max.clamp_(min=0)  #保证self.max大于等于0\n",
    "\n",
    "        if self.min.nelement() == 0  or self.min >tensor.min():\n",
    "            self.min.data = tensor.min().data\n",
    "        self.min.clamp_(max=0)  #保证self.min小于等于0\n",
    "\n",
    "        self.scale,self.zero_point = getScaleZeroPoint(self.min, self.max, self.num_bits)\n",
    "    \n",
    "    def quantize_tensor(self,tensor):\n",
    "        return quantize_tensor(tensor,self.scale,self.zero_point,self.num_bits)\n",
    "        \n",
    "    def dequantize_tensor(self,q_x):\n",
    "        return dequantize_tensor(q_x,self.scale,self.zero_point)\n",
    "\n",
    "    # 从状态字典加载量化参数\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n",
    "        key_names = ['scale', 'zero_point', 'min', 'max']\n",
    "        for key in key_names:\n",
    "            value = getattr(self, key)\n",
    "            value.data = state_dict[prefix + key].data\n",
    "            state_dict.pop(prefix + key)\n",
    "\n",
    "    def __str__(self):\n",
    "        info = 'scale:%.10f '  % self.scale\n",
    "        info += 'zero_point:%d '  % self.zero_point\n",
    "        info += 'min:%.6f '  % self.min\n",
    "        info += 'max:%.6f'  % self.max\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_x= tensor([  0., 229., 255.,  77., 178.])\n",
      "q_parm: scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n",
      "q_parm state dict: OrderedDict([('scale', tensor(0.1310)), ('zero_point', tensor(76.)), ('min', tensor(-10.)), ('max', tensor(23.4000))])\n",
      "q_parm_new scale:0.1309804022 zero_point:76 min:-10.000000 max:23.400000\n"
     ]
    }
   ],
   "source": [
    "q_parm = QParam(num_bits=8)\n",
    "x = torch.tensor([-10.0,20.1,23.4,0.1,13.3])\n",
    "q_parm.update(x)\n",
    "print(\"q_x=\",q_parm.quantize_tensor(x))\n",
    "print(\"q_parm:\",q_parm)  #打印量化参数\n",
    "print(\"q_parm state dict:\",q_parm.state_dict()) #打印状态字典\n",
    "torch.save(q_parm.state_dict(),\"./q_parm.pt\")# 保存状态字典\n",
    "\n",
    "q_parm_new = QParam(num_bits=8)  #创建新的量化参数对象\n",
    "q_parm_new.load_state_dict(torch.load(\"./q_parm.pt\")) #从保存的状态字典中加载量化参数\n",
    "print(\"q_parm_new\",q_parm_new)  #打印量化参数，看是否一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们能够实现对一个tensor进行量化，但仅仅实现了数据层面上的量化。  \n",
    "我们还需要对神经网络的模块和运算进行量化，设置适用于量化的网络层。（conv,relu,maxpooling,fc等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设卷积的权重 weight 为 w，bias 为 b，输入为 x，输出的激活值为 a。由于卷积本质上就是矩阵运算，因此可以表示成:\n",
    "$$ a=\\sum_{i}^{N} w_{i} x_{i}+b$$  \n",
    "量化公式为：\n",
    "$$ S_{a}\\left(q_{a}-Z_{a}\\right)=\\sum_{i}^{N} S_{w}\\left(q_{w}-Z_{w}\\right) S_{x}\\left(q_{x}-Z_{x}\\right)+S_{b}\\left(q_{b}-Z_{b}\\right)$$\n",
    "$$ q_{a}=\\frac{S_{w} S_{x}}{S_{a}} \\sum_{i}^{N}\\left(q_{w}-Z_{w}\\right)\\left(q_{x}-Z_{x}\\right)+\\frac{S_{b}}{S_{a}}\\left(q_{b}-Z_{b}\\right)+Z_{a}$$\n",
    "其中令 $M=\\frac{S_{w} S_{x}}{S_{a}}$ ,一般让$Z_{b}=0$则\n",
    "$$q_{a} = M\\left(\\sum_{i}^{N} q_{w} q_{x}-\\sum_{i}^{N} q_{w} Z_{x}-\\sum_{i}^{N} q_{x} Z_{w}+\\sum_{i}^{N} Z_{w} Z_{x}+q_{b}\\right)+Z_{a}$$\n",
    "从上面可以看出，除了x为动态输入，$q_{w}q_{x}$和$q_{w}Z_{x}$未知，其他的计算结果都可以提前确定下来。  \n",
    "上面除了M是小数，其他都是整数，并且M可以通过bit shift的方法实现定点乘法。  \n",
    "因此上式都可以使用整数定点运算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import math\n",
    "\n",
    "class QModule(nn.Module):\n",
    "    # 创建各种网络模块基类,复用代码\n",
    "\n",
    "    def __init__(self,qi=True,qo=True,num_bits=8):\n",
    "        super().__init__()  #调用父类的构造函数\n",
    "        \"\"\"\n",
    "        网络模块本质是待数据的算子， a = f(x),我们还需要输入x和输出a的量化参数\n",
    "        但并不是算有模块都有输入，所以需要将上一层的qo作为本层的qi\n",
    "\n",
    "        qi:这一层输入的量化参数，包括 S_x,Z_x\n",
    "        qo: 这一层输出的量化参数,包括 S_a,Z_a\n",
    "        \"\"\"\n",
    "        if qi:\n",
    "            self.qi = QParam(num_bits=num_bits)\n",
    "        if qo:\n",
    "            self.qo = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self):\n",
    "        # 将已经能计算出的静态结果冻结下来，并且由浮点实数转为定点整数\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def quantize_inference(self,x):\n",
    "        # 量化推理和正常推理过程不太一致，需要重新编写，因此定义为虚函数。\n",
    "        raise NotImplemented(\"quantize_inference should be implemented.\")\n",
    "\n",
    "class FakeQuantize(Function):\n",
    "    # 伪量化节点，进行量化和反量化\n",
    "    # 模拟量化前后的误差，这样的float推理和量化后的int推理具有相同的精度\n",
    "\n",
    "    # 反向传播求梯度使用STE，这部分在PTQ不进行反向传播，暂时可以忽略。  \n",
    "    # Function类似于没有参数的Module，继承需要重写前向传播forward和反向传播backward\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        \"\"\"\n",
    "        def forward(ctx,input,*args)\n",
    "            ctx: 不需要手动传入，能够执行一些操作方便求梯度，例如：\n",
    "            ctx.save_for_backward(tensor)  在前向传播时保存一些张量\n",
    "            tensor = ctx.saved_tensors    在反向传播时（backward函数内）也可访问到\n",
    "\n",
    "            input:函数的输入\n",
    "            *args:其他可选参数\n",
    "        \"\"\"\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        grad_output:后一层传来的梯度\n",
    "\n",
    "        在QAT涉及到反向传播\n",
    "        对于本层（伪量化节点）,不计算梯度，直接把后一层的梯度往前传\n",
    "\n",
    "        backward的返回值需要和forward对应，代表对应输入的梯度。\n",
    "        由于q_parm为量化参数，不需要计算梯度，因此返回None\n",
    "        因此直接 return grad_output，None\n",
    "        \"\"\"\n",
    "        return grad_output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    # 二维卷积操作的量化版本\n",
    "    def __init__(self,conv_module,qi=True,qo=True,num_bits=8):\n",
    "        # 构造父类的属性\n",
    "        super().__init__(qi=qi,qo=qo,num_bits=num_bits)\n",
    "        self.conv_module = conv_module    #传入未量化的全精度卷积模块\n",
    "        self.qw = QParam(num_bits=num_bits)  #卷积层权重的量化参数\n",
    "        self.num_bits = num_bits\n",
    "        M = torch.tensor([0], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "    \n",
    "    def freeze(self,qi=None,qo=None):\n",
    "        # 为了计算公式中的M，q_w和q_b，并将其冻结\n",
    "\n",
    "        # 量化卷积层要保证qi和qo都存在，且只被初始化过一次。\n",
    "        if hasattr(self,'qi') and qi is not None:\n",
    "            raise ValueError(\"qi has been provided in init function.\")\n",
    "\n",
    "        if not hasattr(self,'qi') and qi in None:\n",
    "            raise ValueError(\"qi is not existed, should be provided.\")\n",
    "        \n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "        \n",
    "        if qi: self.qi = qi\n",
    "        if qo: self.qo = qo\n",
    "\n",
    "        # M = S_w*S_x / S_a \n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        \n",
    "        # 将卷积核参数q_w 量化为定点整数存储\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) \n",
    "        #  为什么减去zero_point? 其实就是对应公式里 q_w-Z_w\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        # 为了方便，使用S_w*S_x来代替S_b\n",
    "        # 对bias使用对称量化，Z_b=0 (实数中的0和量化后的0相同)\n",
    "        # 由于卷积运算结果通常使用32bit存储，因此bias也使用32位量化\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data,scale=self.qi.scale*self.qw.scale,zero_point=0,num_bits=32,signed=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 伪量化前向推理函数，适用于QAT中反向传播\n",
    "        # 推理过程中顺便统计计算输入x，输出a，权重w的量化参数\n",
    "        # 后量化通过数据校准实现这些。\n",
    "\n",
    "        if hasattr(self,'qi'):\n",
    "            self.qi.update(x)  #更新q_x的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qi)  #对q_x进行伪量化\n",
    "\n",
    "        self.qw.update(self.conv_module.weight.data) #更新q_w的量化参数\n",
    "\n",
    "        # 对卷积权重q_w进行伪量化，然后和x计算卷积操作\n",
    "        # 注意卷积模块的权重始终是存储原始的weight，只有在前向传播时进行伪量化。\n",
    "        # 反向传播时也是更新的量化前的weight\n",
    "        x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, \n",
    "                     stride=self.conv_module.stride,\n",
    "                     padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                     groups=self.conv_module.groups)\n",
    "\n",
    "        x = self.conv_module(x)\n",
    "\n",
    "        if hasattr(self,'op'):\n",
    "            self.qo.update(x) #更新输出q_a的量化参数\n",
    "            x = FakeQuantize.apply(x, self.qo)  #对输出a做伪量化\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self,x):\n",
    "        # 将权重和激活值量化后的推理\n",
    "        # 因为pytorch平台限制，这里使用float存储整数，进行浮点运算。\n",
    "        # 实际部署时，应该所有数据和运算都采用定点整数（计算）\n",
    "        x = x - self.qi.zero_point  #q_x - Z_x\n",
    "        x = self.conv_module(x)    #(q_w-Z_w)*(q_x-Z_x)\n",
    "        x = self.M * x             #M*sum((q_w-Z_w)*(q_x-Z_x))\n",
    "        x.round_()                 #提前做一步round，加快速度。后面只剩下一个整数加法。\n",
    "        x = x + self.qo.zero_point   #M*sum((q_w-Z_w)*(q_x-Z_x))+Z_a\n",
    "        x.round_()\n",
    "        return x\n",
    "\n",
    "# 相同原理 实现其他网络模块\n",
    "class QLinear(QModule):\n",
    "    #量化线性层\n",
    "    def __init__(self, fc_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        M = torch.tensor([0], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        self.fc_module.weight.data = self.qw.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.fc_module.weight.data - self.qw.zero_point\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.qi.scale * self.qw.scale,\n",
    "                                                   zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        self.qw.update(self.fc_module.weight.data)\n",
    "\n",
    "        x = F.linear(x, FakeQuantize.apply(self.fc_module.weight, self.qw), self.fc_module.bias)\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()\n",
    "        return x\n",
    "\n",
    "\n",
    "class QReLU(QModule):\n",
    "    # 构建量化版的Relu函数\n",
    "    def __init__(self, qi=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(qi=qi, num_bits=num_bits)\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        \n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.qi.zero_point] = self.qi.zero_point\n",
    "        return x\n",
    "\n",
    "class QMaxPooling2d(QModule):\n",
    "    # 构建量化版的MaxPooling2d函数\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, qi=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(qi=qi, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def freeze(self, qi=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv-BN-Relu融合层的实现\n",
    "一个标准的Conv+BN+relu可以合并到一起，加快推理速度。\n",
    "- **BN折叠到卷积层**  \n",
    "卷积层的输出如下所示:  $$y=\\sum_{i}^{N} w_{i} x_{i}+b$$\n",
    "BN层的输出如下所示:  \n",
    "$$y_{bn} =\\gamma \\frac{y-\\mu_{y}}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}+\\beta$$\n",
    "将卷积层的输出$y$带入到BN层中得到：  \n",
    "$$y_{b n}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}\\left(\\sum_{i}^{N} w_{i} x_{i}+b-\\mu_{y}\\right)+\\beta$$\n",
    "仔细观察，当训练结束后，BN层统计量$\\mu_{y}$ ,$\\sigma_{y}$以及参数$\\gamma$, $\\beta$都已经固定下来。我们可以令$\\gamma^{\\prime}=\\frac{\\gamma}{\\sqrt{\\sigma_{y}^{2}+\\epsilon}}$,那么能够得到：\n",
    "$$y_{b n}=\\sum_{i}^{N} \\gamma^{\\prime} w_{i} x_{i}+\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$$\n",
    "上式已经和卷积计算公式非常像了，为了看得更清楚，我们令$w_{i}^{\\prime} = \\gamma^{\\prime}w_{i}$,  $b^{\\prime}=\\gamma^{\\prime}\\left(b-\\mu_{y}\\right)+\\beta$,得到最终的BN层输出\n",
    " $$y_{bn}=\\sum_{i}^{N} w_{i} x_{i}^{\\prime}+b^{\\prime}$$\n",
    "这就和卷积的操作一模一样，因此我们可以把BN层折叠合并到卷积层中。先进行数值变换，在进行矩阵运算。\n",
    "\n",
    "- **Relu折叠到卷积层**  \n",
    "1. 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。  \n",
    "2. 要想保证浮点型Relu和量化型Relu数值的一致性，需要对输入输出使用相同的量化参数,否则无法反量化回正确的实数域。  \n",
    "3. relu会对数值进行截断，实际上在量化过程中也存在截断，就是把scale和偏移zero point后的值截断到（qmin,qmax）。  \n",
    "4. 对于单独的一个Relu层，我们可以直接使用输入的qi在量化数值空间做截断，$relu(q_{x}) = max(q_{x},Z_{x})$。\n",
    "5. 对于合并到conv中的relu层，我们可以使用**输出的qo作为量化参数，对relu的输入x进行量化**，量化的截断过程可以直接实现relu的操作。这是因为在float域，relu的值域是[0,r_max]，对应的量化值域就是[q_min,q_max]。relu函数在float域中，对小于0的数置为0；在量化int域中，就是对小于q_min的数置为q_min。而使用qo对x进行量化，就能保证这一点。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    # 构建量化BN层（与Relu融合）\n",
    "    def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.qw = QParam(num_bits=num_bits)\n",
    "        self.qb = QParam(num_bits=32)\n",
    "        M = torch.tensor([0], requires_grad=False) # 将M注册为buffer\n",
    "        self.register_buffer('M', M)  \n",
    "\n",
    "    def fold_bn(self, mean, std):\n",
    "        # 将BN层折叠到Conv层\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std  #r' = r/ sqrt(sigma^2+e)  实际上除以的标准差，加上e是为了避免分母为零\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) #w' = r'*w\n",
    "            if self.conv_module.bias is not None:         # b' = r' * b - r' * mu_y + beta\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias    #返回新的w和b\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if hasattr(self, 'qi'):\n",
    "            self.qi.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qi)\n",
    "\n",
    "        # 训练过程中，更新BN层参数\n",
    "        if self.training:\n",
    "            y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, \n",
    "                            stride=self.conv_module.stride,\n",
    "                            padding=self.conv_module.padding,\n",
    "                            dilation=self.conv_module.dilation,\n",
    "                            groups=self.conv_module.groups)\n",
    "            y = y.permute(1, 0, 2, 3) # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -> C,NHW\n",
    "            # mean = y.mean(1)\n",
    "            # var = y.var(1)\n",
    "            mean = y.mean(1).detach()\n",
    "            var = y.var(1).detach()\n",
    "            self.bn_module.running_mean = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = \\\n",
    "                self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                (1 - self.bn_module.momentum) * var\n",
    "        else:\n",
    "            mean = Variable(self.bn_module.running_mean)\n",
    "            var = Variable(self.bn_module.running_var)\n",
    "\n",
    "        std = torch.sqrt(var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(mean, std)  #折叠后的w和b\n",
    "\n",
    "        self.qw.update(weight.data)\n",
    "\n",
    "        #这里的卷积运算实际上包含了BN层\n",
    "        x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, \n",
    "                stride=self.conv_module.stride,\n",
    "                padding=self.conv_module.padding, dilation=self.conv_module.dilation, \n",
    "                groups=self.conv_module.groups)\n",
    "\n",
    "        x = F.relu(x)    #后面接上relu函数\n",
    "\n",
    "        if hasattr(self, 'qo'):\n",
    "            self.qo.update(x)\n",
    "            x = FakeQuantize.apply(x, self.qo)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze(self, qi=None, qo=None):\n",
    "        if hasattr(self, 'qi') and qi is not None:\n",
    "            raise ValueError('qi has been provided in init function.')\n",
    "        if not hasattr(self, 'qi') and qi is None:\n",
    "            raise ValueError('qi is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'qo') and qo is not None:\n",
    "            raise ValueError('qo has been provided in init function.')\n",
    "        if not hasattr(self, 'qo') and qo is None:\n",
    "            raise ValueError('qo is not existed, should be provided.')\n",
    "\n",
    "        if qi is not None:\n",
    "            self.qi = qi\n",
    "        if qo is not None:\n",
    "            self.qo = qo\n",
    "\n",
    "        self.M.data = (self.qw.scale*self.qi.scale / self.qo.scale).data\n",
    "        # self.M = self.qw.scale * self.qi.scale / self.qo.scale\n",
    "\n",
    "        std = torch.sqrt(self.bn_module.running_var + self.bn_module.eps)\n",
    "\n",
    "        weight, bias = self.fold_bn(self.bn_module.running_mean, std)\n",
    "        self.conv_module.weight.data = self.qw.quantize_tensor(weight.data)\n",
    "        self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point\n",
    "\n",
    "        self.conv_module.bias.data = quantize_tensor(bias, scale=self.qi.scale * self.qw.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.qi.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_() \n",
    "        x = x + self.qo.zero_point\n",
    "        \n",
    "        #推理时将Relu合并到量化过程\n",
    "        #这一行包含了relu函数的作用，将x截断到(qmin,qmax)非对称量化         \n",
    "        x.clamp_(0., 2.**self.num_bits-1.).round_()  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型的构建\n",
    "我们在上面已经得到了基本的网络模块（卷积、池化、线性层等）的量化版本，  \n",
    "现在我们尝试构建一个完整的带有BN层的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    # 定义简单的卷积神经网络，包含两层卷积、2层BN层，1层全连接层\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    # 普通的全精度前向传播，穿插了2次relu函数和最大池化操作\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 我们需要把卷积层、池化层、全连接层转为其量化版本\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits)\n",
    "\n",
    "    # 量化版本的前向传播，就是各量化网络模块forward的串行堆叠\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    # 对各种量化参数进行固定\n",
    "    # 除了第一层卷积需要qi，其他都是对上一层的qo复用为当前层的qi，因为上一层的输出就是这一层的输入。\n",
    "    # 这里可以优化，伪量化过程中，这一层输出与下一层输入是一个tensor，但是执行了两次统计。\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.qo)\n",
    "        self.qconv2.freeze(qi=self.qconv1.qo)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.qo)\n",
    "        self.qfc.freeze(qi=self.qconv2.qo)\n",
    "\n",
    "    # 实际推理中用到的函数，是对各量化模块的quantize_inference的堆叠，纯整数存储和运算\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.qi.quantize_tensor(x) #对最开始的float输入进行量化\n",
    "\n",
    "        \"\"\"\n",
    "        中间推理都是在int域进行， 配合量化参数进行前向传播。\n",
    "        伪量化是在float域进行，虽有本质不同，但最终与int域反量化结果一致。\n",
    "        \"\"\"\n",
    "        qx = self.qconv1.quantize_inference(qx)    \n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.qo.dequantize_tensor(qx)   #最后一步要反量化，得到实数域的结果\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全精度模型训练\n",
    "这里直接使用仓库提供的 *train.py* 得到在mnist数据集上准确率为99%的FP32模型，权重保存在 *./ckpt/mnist_cnnbn.pt*  \n",
    "```shell\n",
    "python ./train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 后训练量化（PTQ）\n",
    "这里会将一个全精度模型转为一个量化模型，并且测试其精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的全精度模型\n",
    "fp32_model = NetBN()\n",
    "fp32_model.load_state_dict(torch.load(\"./ckpt/mnist_cnnbn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是几个用于测试精度的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一定的图片数据进行校准，确定input和中间activation的量化参数\n",
    "# 实际上模型weight的校准并不需要数据，这里为了方便都放在quantize_forward中进行校准\n",
    "from torchvision import datasets,transforms\n",
    "test_batch_size = 64\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def direct_quantize(model, test_loader):\n",
    "    # 直接量化，使用一些数据集来校准量化参数,使用200个batch的数据\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_forward(data)\n",
    "        if i % 200 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    # 全精度模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def quantize_inference(model, test_loader):\n",
    "    # 量化模型测试准确率\n",
    "    correct = 0\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        # data,target = data.to(device),target.to(device)\n",
    "        output = model.quantize_inference(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp32_model.eval()\n",
    "full_inference(fp32_model,test_loader)  #测试全精度的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n",
      "num_bits 1\n",
      "\n",
      "Test set: Quant Model Accuracy: 10%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 2\n",
      "\n",
      "Test set: Quant Model Accuracy: 30%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 3\n",
      "\n",
      "Test set: Quant Model Accuracy: 81%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 4\n",
      "\n",
      "Test set: Quant Model Accuracy: 98%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 5\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 6\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 7\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n",
      "direct quantization finish\n",
      "num_bits 8\n",
      "\n",
      "Test set: Quant Model Accuracy: 99%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "accuracy_list = []\n",
    "for num_bits in range(1,9):  #测试num_bits=[1-8]的准确率\n",
    "    model = copy.deepcopy(fp32_model)\n",
    "    model.quantize(num_bits=num_bits) # 将模型的各个网络模块转为其量化版本\n",
    "    model.eval()\n",
    "    direct_quantize(model,test_loader)  #进行量化校准\n",
    "    model.freeze()                       # 将量化参数固定\n",
    "    print(\"num_bits\",num_bits)\n",
    "    acc = quantize_inference(model, test_loader)  #测试量化准确率\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n存在bug， 保存的量化模型无法直接加载.\\n因为量化参数需要前向推理时才会确定。\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "acc_quants = pd.Series(accuracy_list,index=list(range(1,9)))\n",
    "acc_quants.plot()  #不同num_bits下的量化准确率\n",
    "\"\"\"\n",
    "存在bug， 保存的量化模型无法直接加载.\n",
    "因为量化参数需要前向推理时才会确定。\n",
    "\"\"\"\n",
    "# torch.save(model.state_dict(),\"./quan_cnnbn_int8.pt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，当num_bits=4时，量化基本已经达到最大准确率99%。  \n",
    "这说明使用fp32存储这个小模型实在是浪费，使用int4即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.量化感知训练（QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 量化模型训练的问题\n",
    "量化模型我们插入了伪量化节点，经过反量化和量化，能够模拟量化带来的数值误差，实际存储的是反量化的float数值。  \n",
    "但是这样的模型还是不能训练，因为在计算图中，量化操作存在round函数（取整函数），这个函数**梯度几乎处处为0**。  \n",
    "这样我们就会导致反向传播的中梯度为0（链式法则），无法完成训练。  \n",
    "我们可以使用STE尝试解决这个问题。  \n",
    "#### Straight Through Estimator\n",
    "STE方式是，直接跳过伪量化的过程，避开round操作。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。  \n",
    "关键点：对weight伪量化会带来误差，从而降低模型精度。通过反向传播把梯度传给伪量化前的weight，并且**更新伪量化前的weight**， 使其适应量化带来的误差。这就是量化感知训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化感知训练的过程，和普通训练模型没什么差异\n",
    "# 更新的也是量化前weight，伪量化操作只在前向传播中使用引入量化误差，本身并不会修改weight\n",
    "def quantize_aware_training(model, device, train_loader, optimizer, epoch):\n",
    "    lossLayer = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.quantize_forward(data)\n",
    "        loss = lossLayer(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Quantize Aware Training Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize Aware Training Epoch: 1 [3200/60000]\tLoss: 0.228532\n",
      "Quantize Aware Training Epoch: 1 [6400/60000]\tLoss: 0.213571\n",
      "Quantize Aware Training Epoch: 1 [9600/60000]\tLoss: 0.389365\n",
      "Quantize Aware Training Epoch: 1 [12800/60000]\tLoss: 0.214821\n",
      "Quantize Aware Training Epoch: 1 [16000/60000]\tLoss: 0.197394\n",
      "Quantize Aware Training Epoch: 1 [19200/60000]\tLoss: 0.303319\n",
      "Quantize Aware Training Epoch: 1 [22400/60000]\tLoss: 0.222954\n",
      "Quantize Aware Training Epoch: 1 [25600/60000]\tLoss: 0.295887\n",
      "Quantize Aware Training Epoch: 1 [28800/60000]\tLoss: 0.437553\n",
      "Quantize Aware Training Epoch: 1 [32000/60000]\tLoss: 0.331082\n",
      "Quantize Aware Training Epoch: 1 [35200/60000]\tLoss: 0.228991\n",
      "Quantize Aware Training Epoch: 1 [38400/60000]\tLoss: 0.405622\n",
      "Quantize Aware Training Epoch: 1 [41600/60000]\tLoss: 0.225090\n",
      "Quantize Aware Training Epoch: 1 [44800/60000]\tLoss: 0.101532\n",
      "Quantize Aware Training Epoch: 1 [48000/60000]\tLoss: 0.351024\n",
      "Quantize Aware Training Epoch: 1 [51200/60000]\tLoss: 0.291945\n",
      "Quantize Aware Training Epoch: 1 [54400/60000]\tLoss: 0.084747\n",
      "Quantize Aware Training Epoch: 1 [57600/60000]\tLoss: 0.226448\n",
      "Quantize Aware Training Epoch: 2 [3200/60000]\tLoss: 0.248598\n",
      "Quantize Aware Training Epoch: 2 [6400/60000]\tLoss: 0.532002\n",
      "Quantize Aware Training Epoch: 2 [9600/60000]\tLoss: 0.176943\n",
      "Quantize Aware Training Epoch: 2 [12800/60000]\tLoss: 0.185655\n",
      "Quantize Aware Training Epoch: 2 [16000/60000]\tLoss: 0.282046\n",
      "Quantize Aware Training Epoch: 2 [19200/60000]\tLoss: 0.402830\n",
      "Quantize Aware Training Epoch: 2 [22400/60000]\tLoss: 0.428578\n",
      "Quantize Aware Training Epoch: 2 [25600/60000]\tLoss: 0.346151\n",
      "Quantize Aware Training Epoch: 2 [28800/60000]\tLoss: 0.424758\n",
      "Quantize Aware Training Epoch: 2 [32000/60000]\tLoss: 0.262605\n",
      "Quantize Aware Training Epoch: 2 [35200/60000]\tLoss: 0.168451\n",
      "Quantize Aware Training Epoch: 2 [38400/60000]\tLoss: 0.317902\n",
      "Quantize Aware Training Epoch: 2 [41600/60000]\tLoss: 0.236154\n",
      "Quantize Aware Training Epoch: 2 [44800/60000]\tLoss: 0.223695\n",
      "Quantize Aware Training Epoch: 2 [48000/60000]\tLoss: 0.242803\n",
      "Quantize Aware Training Epoch: 2 [51200/60000]\tLoss: 0.225980\n",
      "Quantize Aware Training Epoch: 2 [54400/60000]\tLoss: 0.619919\n",
      "Quantize Aware Training Epoch: 2 [57600/60000]\tLoss: 0.303927\n",
      "Quantize Aware Training Epoch: 3 [3200/60000]\tLoss: 0.281351\n",
      "Quantize Aware Training Epoch: 3 [6400/60000]\tLoss: 0.464758\n",
      "Quantize Aware Training Epoch: 3 [9600/60000]\tLoss: 0.271200\n",
      "Quantize Aware Training Epoch: 3 [12800/60000]\tLoss: 0.651713\n",
      "Quantize Aware Training Epoch: 3 [16000/60000]\tLoss: 0.370171\n",
      "Quantize Aware Training Epoch: 3 [19200/60000]\tLoss: 0.313809\n",
      "Quantize Aware Training Epoch: 3 [22400/60000]\tLoss: 0.365523\n",
      "Quantize Aware Training Epoch: 3 [25600/60000]\tLoss: 0.507615\n",
      "Quantize Aware Training Epoch: 3 [28800/60000]\tLoss: 0.286144\n",
      "Quantize Aware Training Epoch: 3 [32000/60000]\tLoss: 0.241107\n",
      "Quantize Aware Training Epoch: 3 [35200/60000]\tLoss: 0.403052\n",
      "Quantize Aware Training Epoch: 3 [38400/60000]\tLoss: 0.273703\n",
      "Quantize Aware Training Epoch: 3 [41600/60000]\tLoss: 0.561951\n",
      "Quantize Aware Training Epoch: 3 [44800/60000]\tLoss: 0.282516\n",
      "Quantize Aware Training Epoch: 3 [48000/60000]\tLoss: 0.294975\n",
      "Quantize Aware Training Epoch: 3 [51200/60000]\tLoss: 0.525729\n",
      "Quantize Aware Training Epoch: 3 [54400/60000]\tLoss: 0.178631\n",
      "Quantize Aware Training Epoch: 3 [57600/60000]\tLoss: 0.248051\n"
     ]
    }
   ],
   "source": [
    "# 进行量化感知训练\n",
    "from torch import optim\n",
    "batch_size = 64\n",
    "seed = 1\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, \n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False\n",
    ")\n",
    "\n",
    "model_qat = copy.deepcopy(fp32_model)  #从全精度模型获得量化模型\n",
    "model_qat.quantize(num_bits=3)  #量化位数为3\n",
    "model_qat.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "model_qat.train()\n",
    "for epoch in range(epochs):\n",
    "    quantize_aware_training(model_qat,device,train_loader,optimizer,epoch+1)\n",
    "model_qat.eval()\n",
    "model_qat.freeze() #冻结量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Quant Model Accuracy: 80%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79.92"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat.cpu()\n",
    "quantize_inference(model_qat, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在num_bits=3时，PTQ就有84%的准确率，而QAT只有74%的准确率，还不如直接量化呢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.网络性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size =512\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, num_workers=16, pin_memory=True\n",
    ")  #pin_memory为True时，生成的tensor数据在锁页内存中（而不会在虚拟内存中 ），转移到gpu会更快。内存宽裕的情况下建议设为True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## num_workers的作用\n",
    "设置多线程处理，提前把num_workers个batch加载到内存中，增大能够显著提升数据加载速度。\n",
    "下面测试的实验：在bs=512的情况下，将num_workers从1调到了16，gpu利用率从3%提升到85%。\n",
    "运行时间从64000us减少到19085.\n",
    "other利用时间显著减少，从中可以看出other时间大概率就是IO数据加载时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct quantization finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "profiling...: 100%|██████████| 20/20 [00:05<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# 构建追踪的属性\n",
    "model = NetBN()\n",
    "model.quantize(num_bits=8)\n",
    "direct_quantize(model,test_loader=test_loader)\n",
    "model.freeze()\n",
    "model.load_state_dict(torch.load(\"./quan_cnnbn_int8.pt\"))\n",
    "model.eval() \n",
    "model.cuda()\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    schedule = torch.profiler.schedule(wait=2,warmup=2,active=6,repeat=1),\n",
    "    on_trace_ready = torch.profiler.tensorboard_trace_handler(dir_name=\"./performance/\"),  #日志保存地址\n",
    "    activities = [\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA\n",
    "    ],\n",
    "    with_stack = True,\n",
    ") as profiler:\n",
    "    with torch.no_grad():\n",
    "        for data,target in tqdm(test_loader,desc=\"profiling...\"):  #这里用假数据也行，不考虑模型预测精度\n",
    "            model.quantize_inference(data.cuda())\n",
    "            profiler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1f49b93815805eca9ccc5299d655a62f3a8d0678e274dc3dfeb518f21176dcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
